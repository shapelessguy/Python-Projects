{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24627f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import threading\n",
    "import time\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f830031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "        'max_new_tokens': 10,\n",
    "        'history': {'internal': [], 'visible': []},\n",
    "        'mode': 'instruct',  # Valid options: 'chat', 'chat-instruct', 'instruct'\n",
    "        'character': 'Example',\n",
    "        'instruction_template': 'Vicuna-v1.1',\n",
    "        'your_name': 'You',\n",
    "\n",
    "        'regenerate': False,\n",
    "        '_continue': False,\n",
    "        'stop_at_newline': False,\n",
    "        'chat_prompt_size': 2048,\n",
    "        'chat_generation_attempts': 1,\n",
    "        'chat-instruct_command': 'Continue the chat dialogue below. Write a single reply for the character \"'\n",
    "                                 '<|character|>\".\\n\\n<|prompt|>',\n",
    "\n",
    "        # Generation params. If 'preset' is set to different than 'None', the values\n",
    "        # in presets/preset-name.yaml are used instead of the individual numbers.\n",
    "        'preset': 'None',\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.1,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.18,\n",
    "        'top_k': 40,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f73a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_settings = {\n",
    "        'max_new_tokens': 10,\n",
    "        'history': {'internal': [], 'visible': []},\n",
    "        'mode': 'instruct',  # Valid options: 'chat', 'chat-instruct', 'instruct'\n",
    "        'character': 'Example',\n",
    "        'instruction_template': 'Vicuna-v1.1',\n",
    "        'your_name': 'You',\n",
    "\n",
    "        'regenerate': False,\n",
    "        '_continue': False,\n",
    "        'stop_at_newline': False,\n",
    "        'chat_prompt_size': 2048,\n",
    "        'chat_generation_attempts': 1,\n",
    "        'chat-instruct_command': 'Continue the chat dialogue below. Write a single reply for the character \"'\n",
    "                                 '<|character|>\".\\n\\n<|prompt|>',\n",
    "        'preset': 'None',\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.1,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.18,\n",
    "        'top_k': 40,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "}\n",
    "\n",
    "\n",
    "def write_response(command):\n",
    "    with open('chat/instructions.json', 'a+') as file:\n",
    "        file.write(command.text.replace('\\n', '\\\\n') + \"\\n\")\n",
    "    with open('chat/responses.json', 'a+') as file:\n",
    "        file.write(command.response.replace('\\n', '\\\\n') + \"\\n\")\n",
    "\n",
    "\n",
    "class Command:\n",
    "    def __init__(self, id_, text, settings):\n",
    "        self.id_ = id_\n",
    "        self.text = text\n",
    "        self.n_words = len(text.split())\n",
    "        self.settings = settings\n",
    "        self.flag_resp = False\n",
    "        self.response = \"\"\n",
    "        self.n_tokens = -1\n",
    "\n",
    "    def def_request(self):\n",
    "        return {\n",
    "            'user_input': self.text,\n",
    "            **self.settings\n",
    "        }\n",
    "\n",
    "    def def_prompt(self):\n",
    "        return {'prompt': self.text}\n",
    "\n",
    "    def set_response(self, response):\n",
    "        self.flag_resp = True\n",
    "        self.response = response['history']['visible'][0][1]\n",
    "        write_response(self)\n",
    "        print(len(self.text), len(self.text.split()), self.n_tokens, '\\n')\n",
    "        print(f'Response to command {self.id_} ({self.n_tokens} tokens) given.', '\\n')\n",
    "\n",
    "    def set_tokens(self, response):\n",
    "        self.n_tokens = response['tokens']\n",
    "\n",
    "\n",
    "class Resource:\n",
    "    def __init__(self, type_, address, available):\n",
    "        self.type = type_\n",
    "        self.address = address\n",
    "        host = address.split(\":\")[0]\n",
    "        port = int(address.split(\":\")[1])\n",
    "        self.api = f'http://{host}:{port + 1000}/api/v1/chat'\n",
    "        self.ct_api = f'http://{host}:{port + 1000}/api/v1/token-count'\n",
    "        self.available = available\n",
    "        self.status = \"free\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.type}, {self.address}, {self.available}, {self.status}'\n",
    "\n",
    "\n",
    "class ResourcePool(list):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.available_resources = []\n",
    "\n",
    "    def insert_resource(self, type_, address, available):\n",
    "        for element in self:\n",
    "            if address == element.address:\n",
    "                element.available = available\n",
    "                return\n",
    "        self.append(Resource(type_, address, available))\n",
    "\n",
    "    def update_avail_resources(self):\n",
    "        self.available_resources = []\n",
    "        for element in self:\n",
    "            if element.available:\n",
    "                self.available_resources.append(element)\n",
    "\n",
    "    def get_avail_resources(self):\n",
    "        return self.available_resources\n",
    "\n",
    "\n",
    "def check_status(signal):\n",
    "    resource_pool = signal[1]\n",
    "    clst_status = {}\n",
    "    while signal[0]:\n",
    "        with open('cluster_management/cluster_status.json', 'r') as file:\n",
    "            clst_status = json.load(file)\n",
    "        for k in clst_status:\n",
    "            props = k.split()\n",
    "            resource_pool.insert_resource(props[0], props[1] + \":\" + props[2], clst_status[k] == 'ON')\n",
    "        resource_pool.update_avail_resources()\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "def req(request, resource):\n",
    "\n",
    "    # Check for prompt overflow\n",
    "    if request[0].n_words > (request[0].settings['chat_prompt_size'] + request[0].settings['max_new_tokens']) / 2:\n",
    "        try:\n",
    "            prompt = request[0].def_prompt()\n",
    "            response = requests.post(resource.ct_api, json=request[0].def_prompt())\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()['results'][0]\n",
    "                request[0].set_tokens(result)\n",
    "                if request[0].n_tokens > request[0].settings['chat_prompt_size'] + request[0].settings['max_new_tokens']:\n",
    "                    request[1] = 'computed'\n",
    "                    print(f'WARNING: command {request[0].id_} too long! ({request[0].n_tokens} tokens)')\n",
    "            else:\n",
    "                print(f'Status code: {response.status_code}')\n",
    "                request[1] = 'to_be_computed'\n",
    "                return\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            request[1] = 'to_be_computed'\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        response = requests.post(resource.api, json=request[0].def_request())\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()['results'][0]\n",
    "            request[0].set_response(result)\n",
    "            request[1] = 'computed'\n",
    "        else:\n",
    "            print(f'Status code: {response.status_code}')\n",
    "            request[1] = 'to_be_computed'\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        request[1] = 'to_be_computed'\n",
    "\n",
    "\n",
    "def compute_queue(inputs, signal):\n",
    "    resource_pool = signal[1]\n",
    "    diagnostic_request = [Command(-1, 'example_text', diagnostic_settings), 'to_be_computed']\n",
    "    all_requests = [[inp, 'to_be_computed'] for inp in inputs]\n",
    "    free_resources = [x for x in resource_pool.get_avail_resources()]\n",
    "    if len(free_resources) == 0:\n",
    "        raise \"NO RESOURCES AVAILABLE\"\n",
    "    busy_resources = []\n",
    "    err_resources = []\n",
    "    tasks = []\n",
    "\n",
    "    while 1:\n",
    "        for resource in free_resources:\n",
    "            for request in all_requests:\n",
    "                if request[1] == 'to_be_computed':\n",
    "                    request[1] = 'computing'\n",
    "                    tasks.append([threading.Thread(target=req, args=(request, resource)), resource, request])\n",
    "                    tasks[-1][0].start()\n",
    "                    resource.status = 'busy'\n",
    "                    busy_resources.append(free_resources.pop(free_resources.index(resource)))\n",
    "                    break\n",
    "        for i in range(len(tasks))[::-1]:\n",
    "            if not tasks[i][0].is_alive():\n",
    "                resource = tasks[i][1]\n",
    "                if tasks[i][-1][1] != 'computed':\n",
    "                    diagnostics = threading.Thread(target=req, args=(diagnostic_request, resource))\n",
    "                    diagnostics.start()\n",
    "                    diagnostics.join()\n",
    "                    if diagnostic_request[1] != 'computed':\n",
    "                        resource.status = 'error'\n",
    "                        err_resources.append(busy_resources.pop(busy_resources.index(resource)))\n",
    "                        print(f\"RESOURCE {resource.api} NOT RESPONDING\")\n",
    "                    else:\n",
    "                        resource.status = 'free'\n",
    "                        free_resources.append(busy_resources.pop(busy_resources.index(resource)))\n",
    "                else:\n",
    "                    resource.status = 'free'\n",
    "                    free_resources.append(busy_resources.pop(busy_resources.index(resource)))\n",
    "                tasks.pop(i)\n",
    "        if sum(1 if x[1] != 'computed' else 0 for x in all_requests) == 0:\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9c15f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (check_status):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\mambaforge_22.9.0.2\\envs\\c_venv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\mambaforge_22.9.0.2\\envs\\c_venv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\cian_cl-a\\AppData\\Local\\Temp\\ipykernel_21460\\1193911141.py\", line 128, in check_status\n",
      "  File \"C:\\Users\\cian_cl-a\\AppData\\Local\\Temp\\ipykernel_21460\\1193911141.py\", line 108, in insert_resource\n",
      "  File \"C:\\Users\\cian_cl-a\\AppData\\Local\\Temp\\ipykernel_21460\\1193911141.py\", line 88, in __init__\n",
      "ValueError: invalid literal for int() with base 10: 'None'\n"
     ]
    }
   ],
   "source": [
    "signal = [True, ResourcePool()]\n",
    "threading.Thread(target=check_status, args=(signal, )).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dced84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b973e8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m compute_queue(\u001b[43minputs\u001b[49m, signal)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "compute_queue(inputs, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ee731",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [Command(i, f\"\"\"\n",
    "### Instruction:\n",
    "Summarize me the following text in one line:\n",
    "\"you are ugly\"\n",
    "\n",
    "### Response:\n",
    "\"\"\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
